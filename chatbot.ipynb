{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0adec47-e3b8-4855-9449-0bbb98ee2ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\vedashree\\anaconda3\\lib\\site-packages (4.49.0)\n",
      "Requirement already satisfied: torch in c:\\users\\vedashree\\anaconda3\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\vedashree\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in c:\\users\\vedashree\\anaconda3\\lib\\site-packages (from transformers) (0.26.5)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\vedashree\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\vedashree\\anaconda3\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\vedashree\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\vedashree\\anaconda3\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\vedashree\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\vedashree\\anaconda3\\lib\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\vedashree\\anaconda3\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\vedashree\\anaconda3\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\vedashree\\anaconda3\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\vedashree\\anaconda3\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\vedashree\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\vedashree\\anaconda3\\lib\\site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\vedashree\\anaconda3\\lib\\site-packages (from torch) (69.5.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\vedashree\\anaconda3\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\vedashree\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\vedashree\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\vedashree\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\vedashree\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\vedashree\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\vedashree\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vedashree\\anaconda3\\lib\\site-packages (from requests->transformers) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "980df601-e943-43de-b7e3-f4f72d16ab54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import tkinter as tk\n",
    "from tkinter import scrolledtext\n",
    "import random\n",
    "import re\n",
    "import wikipedia\n",
    "import requests\n",
    "\n",
    "# Path to the model directory\n",
    "model_path = r\"C:\\Users\\Vedashree\\OneDrive\\Desktop\\DialoGPT-medium\\DialoGPT-master\"\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Move model to GPU if available, otherwise keep it on CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Initialize conversation history to store messages\n",
    "conversation_history = []\n",
    "\n",
    "# Create the main window\n",
    "root = tk.Tk()\n",
    "root.title(\"Comprehensive Chatbot\")\n",
    "root.geometry(\"600x650\")\n",
    "root.configure(bg=\"#2C3E50\")  # Dark blue background\n",
    "\n",
    "# Create a frame for the chat history\n",
    "frame = tk.Frame(root, bg=\"#34495E\", padx=10, pady=10)\n",
    "frame.pack(pady=10, padx=10, fill=tk.BOTH, expand=True)\n",
    "\n",
    "# Create a ScrolledText widget for displaying conversation history\n",
    "chat_history = scrolledtext.ScrolledText(frame, state='disabled', wrap=tk.WORD, height=20, width=58, bg=\"#ECF0F1\", fg=\"#2C3E50\", font=(\"Arial\", 12))\n",
    "chat_history.pack(fill=tk.BOTH, expand=True)\n",
    "\n",
    "# Create a frame for user input\n",
    "input_frame = tk.Frame(root, bg=\"#2C3E50\")\n",
    "input_frame.pack(pady=10, padx=10, fill=tk.X)\n",
    "\n",
    "# Create an Entry widget for user input\n",
    "user_input = tk.Entry(input_frame, width=40, font=(\"Arial\", 12), bg=\"#ECF0F1\", fg=\"#2C3E50\")\n",
    "user_input.pack(side=tk.LEFT, padx=10, pady=10, fill=tk.X, expand=True)\n",
    "\n",
    "def analyze_question_type(query):\n",
    "    \"\"\"Analyze the type of question being asked.\"\"\"\n",
    "    query_lower = query.lower()\n",
    "    \n",
    "    # Factual questions\n",
    "    if any(phrase in query_lower for phrase in [\"what is\", \"who is\", \"where is\", \"when did\", \"how does\", \"why is\", \"tell me about\"]):\n",
    "        return \"factual\"\n",
    "    \n",
    "    # Step-by-step instructions\n",
    "    if any(phrase in query_lower for phrase in [\"how to\", \"steps to\", \"guide\", \"tutorial\", \"instructions\"]):\n",
    "        return \"procedural\"\n",
    "    \n",
    "    # Opinions or subjective questions\n",
    "    if any(phrase in query_lower for phrase in [\"opinion\", \"think about\", \"perspective\", \"viewpoint\", \"better\", \"should i\"]):\n",
    "        return \"subjective\"\n",
    "    \n",
    "    # Comparison questions\n",
    "    if any(phrase in query_lower for phrase in [\"versus\", \"vs\", \"compare\", \"difference between\", \"similarities\"]):\n",
    "        return \"comparison\"\n",
    "    \n",
    "    # Definition questions\n",
    "    if any(phrase in query_lower for phrase in [\"define\", \"meaning of\", \"definition\"]):\n",
    "        return \"definition\"\n",
    "    \n",
    "    return \"conversation\"\n",
    "\n",
    "def analyze_complexity(query):\n",
    "    \"\"\"Analyze the complexity of the query to adapt response detail.\"\"\"\n",
    "    # Count words as a basic complexity measure\n",
    "    word_count = len(query.split())\n",
    "    \n",
    "    if word_count > 12:\n",
    "        return \"high\"\n",
    "    elif word_count > 6:\n",
    "        return \"medium\"\n",
    "    else:\n",
    "        return \"low\"\n",
    "\n",
    "def analyze_and_generate_response(user_message):\n",
    "    \"\"\"Analyzes the question and generates an appropriate response.\"\"\"\n",
    "    user_message = user_message.strip()\n",
    "    user_message_lower = user_message.lower()\n",
    "    \n",
    "    # Detect question type and complexity\n",
    "    question_type = analyze_question_type(user_message)\n",
    "    complexity = analyze_complexity(user_message)\n",
    "    \n",
    "    # Handle greetings and common conversation starters\n",
    "    if re.match(r\"^(hi|hello|hey)$\", user_message_lower):\n",
    "        return random.choice([\n",
    "            \"Hi there! How can I assist you today?\", \n",
    "            \"Hello! I'm here to help. What would you like to know?\",\n",
    "            \"Hey! I'm ready to answer any questions you might have.\"\n",
    "        ])\n",
    "    \n",
    "    if \"how are you\" in user_message_lower:\n",
    "        return \"I'm doing well, thanks for asking! How can I help you today?\"\n",
    "    \n",
    "    # Handle factual questions with Wikipedia\n",
    "    if question_type == \"factual\":\n",
    "        wiki_response = fetch_wikipedia_summary(user_message)\n",
    "        if \"couldn't find any information\" not in wiki_response:\n",
    "            return format_response(wiki_response, complexity)\n",
    "    \n",
    "    # Handle specific queries like weather or news\n",
    "    if \"weather\" in user_message_lower:\n",
    "        return fetch_weather_info(user_message)\n",
    "    elif \"news\" in user_message_lower:\n",
    "        return fetch_news_info()\n",
    "    \n",
    "    # Handle subjective questions with balanced views\n",
    "    if question_type == \"subjective\":\n",
    "        return generate_balanced_response(user_message)\n",
    "    \n",
    "    # Handle procedural questions with step-by-step instructions\n",
    "    if question_type == \"procedural\":\n",
    "        return generate_procedural_response(user_message)\n",
    "    \n",
    "    # Handle comparison questions\n",
    "    if question_type == \"comparison\":\n",
    "        return generate_comparison_response(user_message)\n",
    "    \n",
    "    # Handle definition questions\n",
    "    if question_type == \"definition\":\n",
    "        return generate_definition_response(user_message)\n",
    "    \n",
    "    # Fallback to model-based conversation\n",
    "    return generate_response_from_model(user_message)\n",
    "\n",
    "def format_response(content, complexity):\n",
    "    \"\"\"Format the response based on complexity.\"\"\"\n",
    "    # Adapt language complexity\n",
    "    if complexity == \"high\":\n",
    "        content = content  # Keep detailed explanation\n",
    "    elif complexity == \"medium\":\n",
    "        # Add an explanatory note for medium complexity\n",
    "        if len(content.split()) > 30:\n",
    "            content = content + \"\\n\\nDoes that help answer your question? Let me know if you need more details.\"\n",
    "    else:\n",
    "        # Simplify for low complexity by keeping first couple sentences\n",
    "        sentences = content.split('.')\n",
    "        if len(sentences) > 3:\n",
    "            content = '.'.join(sentences[:3]) + '.'\n",
    "            content += \"\\n\\nI can provide more details if you'd like.\"\n",
    "    \n",
    "    return content\n",
    "\n",
    "def generate_balanced_response(query):\n",
    "    \"\"\"Generate a response for subjective questions with multiple perspectives.\"\"\"\n",
    "    # Extract the main topic from the query\n",
    "    topic = re.sub(r'^(what|who|where|when|why|how|should|could|would|is|are|do|does|did|can|could) ', '', query.lower())\n",
    "    topic = re.sub(r'\\?$', '', topic)\n",
    "    \n",
    "    response = f\"When it comes to {topic}, there are several perspectives to consider:\\n\\n\"\n",
    "    response += \"On one hand, some might argue that this approach has benefits such as improved efficiency and broader accessibility.\\n\\n\"\n",
    "    response += \"On the other hand, others might point out potential concerns including long-term sustainability and ethical considerations.\\n\\n\"\n",
    "    response += \"A balanced view would acknowledge both sides while considering your specific context and priorities.\"\n",
    "    \n",
    "    return response\n",
    "\n",
    "def generate_procedural_response(query):\n",
    "    \"\"\"Generate step-by-step instructions for procedural questions.\"\"\"\n",
    "    # Extract the topic from the query\n",
    "    match = re.search(r\"how to (.*?)(?:\\?|$)\", query.lower())\n",
    "    topic = match.group(1) if match else query\n",
    "    \n",
    "    steps = [\n",
    "        f\"First, understand the basics of {topic} by gathering necessary information.\",\n",
    "        f\"Next, prepare all required tools and materials for {topic}.\",\n",
    "        f\"Then, start with the most fundamental step of {topic}, which typically involves initial setup.\",\n",
    "        f\"As you progress, pay attention to details and make adjustments as needed.\",\n",
    "        f\"Finally, review your work and make any necessary refinements.\"\n",
    "    ]\n",
    "    \n",
    "    response = f\"Here's a step-by-step guide for {topic}:\\n\\n\"\n",
    "    for i, step in enumerate(steps, 1):\n",
    "        response += f\"{i}. {step}\\n\"\n",
    "    \n",
    "    response += f\"\\nIf you run into any issues with this approach, there are alternative methods you could try. Would you like me to explain any of these steps in more detail?\"\n",
    "    \n",
    "    return response\n",
    "\n",
    "def generate_comparison_response(query):\n",
    "    \"\"\"Generate comparison between items/concepts.\"\"\"\n",
    "    # Try to extract items being compared\n",
    "    items = re.findall(r\"between (.*?) and (.*?)(?:\\?|$)\", query.lower())\n",
    "    if not items:\n",
    "        return generate_response_from_model(query)\n",
    "    \n",
    "    item1, item2 = items[0]\n",
    "    \n",
    "    response = f\"When comparing {item1} and {item2}:\\n\\n\"\n",
    "    response += f\"Similarities:\\n- Both serve valuable purposes in their respective contexts\\n- Both have developed over time with increasing sophistication\\n\\n\"\n",
    "    response += f\"Key differences:\\n- {item1.capitalize()} tends to be more focused on specific applications\\n- {item2.capitalize()} often offers different advantages in terms of flexibility and scope\\n\\n\"\n",
    "    response += f\"The choice between {item1} and {item2} ultimately depends on your specific needs and circumstances.\"\n",
    "    \n",
    "    return response\n",
    "\n",
    "def generate_definition_response(query):\n",
    "    \"\"\"Generate definition for terms.\"\"\"\n",
    "    # Extract the term to define\n",
    "    match = re.search(r\"(?:define|what is|meaning of) (.*?)(?:\\?|$)\", query.lower())\n",
    "    term = match.group(1) if match else query\n",
    "    \n",
    "    # Try Wikipedia first\n",
    "    wiki_response = fetch_wikipedia_summary(term)\n",
    "    if \"couldn't find any information\" not in wiki_response:\n",
    "        return f\"{term.capitalize()} refers to {wiki_response}\"\n",
    "    \n",
    "    # Fallback to generated response\n",
    "    response = f\"{term.capitalize()} typically refers to a concept that \"\n",
    "    response += random.choice([\n",
    "        \"involves interconnected elements working together to achieve a specific purpose.\",\n",
    "        \"describes a process or method used in various contexts.\",\n",
    "        \"represents an important principle or idea in its field.\"\n",
    "    ])\n",
    "    \n",
    "    return response\n",
    "\n",
    "def fetch_wikipedia_summary(query):\n",
    "    \"\"\"Fetch a comprehensive summary from Wikipedia API.\"\"\"\n",
    "    # Extract the main subject from the query\n",
    "    search_terms = re.sub(r'^(what is|who is|tell me about|how does|when is|where is|why is|define|meaning of) ', '', query.lower())\n",
    "    search_terms = re.sub(r'\\?$', '', search_terms)\n",
    "    \n",
    "    try:\n",
    "        # Search Wikipedia for the query\n",
    "        search_results = wikipedia.search(search_terms, results=3)\n",
    "        if not search_results:\n",
    "            return \"I'm sorry, I couldn't find any information on that topic. You might want to check a specialized resource or rephrase your question.\"\n",
    "        \n",
    "        # Try to get the most relevant page\n",
    "        try:\n",
    "            page = wikipedia.page(search_results[0])\n",
    "        except wikipedia.exceptions.DisambiguationError as e:\n",
    "            # If disambiguation page, take the first option\n",
    "            page = wikipedia.page(e.options[0])\n",
    "        \n",
    "        # Get a longer summary (3-4 sentences)\n",
    "        summary = wikipedia.summary(page.title, sentences=4)\n",
    "        \n",
    "        # Add a source attribution\n",
    "        summary += f\"\\n\\nThis information comes from Wikipedia's article on '{page.title}'.\"\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    except wikipedia.exceptions.PageError:\n",
    "        return \"I'm sorry, I couldn't find specific information on that. Consider checking specialized resources for more details.\"\n",
    "    except Exception as e:\n",
    "        return f\"I encountered an issue while searching for information. You might want to try rephrasing your question or consult a specialized resource.\"\n",
    "\n",
    "def fetch_weather_info(query):\n",
    "    \"\"\"Fetch weather info based on user input with better city extraction.\"\"\"\n",
    "    # More robust city extraction\n",
    "    city_match = re.search(r'weather (?:in|for|at) ([A-Za-z\\s]+)', query)\n",
    "    city = city_match.group(1).strip() if city_match else extract_city_from_query(query)\n",
    "    \n",
    "    if city:\n",
    "        return get_weather_data(city)\n",
    "    return \"Please specify a city for the weather information. For example, 'What's the weather in New York?'\"\n",
    "\n",
    "def get_weather_data(city):\n",
    "    \"\"\"Fetch weather data using an API (replace with an actual API call).\"\"\"\n",
    "    # This is a placeholder. In a real implementation, you would use a weather API\n",
    "    weather_conditions = [\"sunny\", \"partly cloudy\", \"overcast\", \"rainy\", \"stormy\", \"snowy\"]\n",
    "    temperatures = range(0, 35)\n",
    "    \n",
    "    condition = random.choice(weather_conditions)\n",
    "    temp = random.choice(temperatures)\n",
    "    \n",
    "    response = f\"The current weather in {city} is {condition} with a temperature of {temp}Â°C.\"\n",
    "    \n",
    "    # Add a practical suggestion based on weather\n",
    "    if condition == \"sunny\" and temp > 25:\n",
    "        response += \" It's quite warm, so remember to stay hydrated if you're going outside.\"\n",
    "    elif condition == \"rainy\":\n",
    "        response += \" Don't forget your umbrella if you're heading out!\"\n",
    "    elif condition == \"snowy\" or temp < 5:\n",
    "        response += \" Bundle up, it's cold outside!\"\n",
    "    \n",
    "    response += \"\\n\\nNote: This is simulated weather data. For accurate forecasts, please check a weather service.\"\n",
    "    \n",
    "    return response\n",
    "\n",
    "def fetch_news_info():\n",
    "    \"\"\"Fetch top news headlines with categorization.\"\"\"\n",
    "    # This is a placeholder. In a real implementation, you would use a news API\n",
    "    headlines = [\n",
    "        \"New Study Reveals Benefits of Regular Exercise\",\n",
    "        \"Tech Company Unveils Latest Smartphone Model\",\n",
    "        \"Scientists Make Breakthrough in Renewable Energy\",\n",
    "        \"Global Leaders Meet to Discuss Climate Change\",\n",
    "        \"Stock Markets Show Strong Performance This Week\",\n",
    "        \"New Film Receives Critical Acclaim at Festival\"\n",
    "    ]\n",
    "    \n",
    "    response = \"Here are some recent headlines:\\n\\n\"\n",
    "    for headline in headlines:\n",
    "        response += f\"- {headline}\\n\"\n",
    "    \n",
    "    response += \"\\nNote: These are simulated headlines. For current news, please check a news service.\"\n",
    "    \n",
    "    return response\n",
    "\n",
    "def extract_city_from_query(query):\n",
    "    \"\"\"Extract city name from the user's query with an expanded list.\"\"\"\n",
    "    # Expanded city list\n",
    "    cities = [\"New York\", \"London\", \"Paris\", \"Tokyo\", \"Beijing\", \"Sydney\", \"Moscow\", \n",
    "              \"Cairo\", \"Mumbai\", \"Rio de Janeiro\", \"Toronto\", \"Berlin\", \"Rome\", \"Madrid\"]\n",
    "    \n",
    "    query_lower = query.lower()\n",
    "    for city in cities:\n",
    "        if city.lower() in query_lower:\n",
    "            return city\n",
    "    return None\n",
    "\n",
    "def generate_response_from_model(user_message):\n",
    "    \"\"\"Generate a response using the DialoGPT model.\"\"\"\n",
    "    global conversation_history\n",
    "    conversation_history.append(f\"You: {user_message}\")\n",
    "    \n",
    "    # Limit conversation history to prevent token overflow\n",
    "    if len(conversation_history) > 10:\n",
    "        conversation_history = conversation_history[-10:]\n",
    "    \n",
    "    input_text = \" \".join(conversation_history[-6:])\n",
    "    input_ids = tokenizer.encode(input_text + tokenizer.eos_token, return_tensors='pt').to(device)\n",
    "    \n",
    "    # Generate response with better parameters for more coherent output\n",
    "    output_ids = model.generate(\n",
    "        input_ids, \n",
    "        max_length=150, \n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        attention_mask=torch.ones(input_ids.shape, device=input_ids.device),\n",
    "        top_p=0.92, \n",
    "        top_k=50, \n",
    "        temperature=0.7, \n",
    "        no_repeat_ngram_size=3,\n",
    "        do_sample=True, \n",
    "        num_return_sequences=1\n",
    "    )\n",
    "    \n",
    "    bot_response = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    bot_response = bot_response[len(input_text):].strip()\n",
    "    \n",
    "    # Clean up the response if it's empty or too short\n",
    "    if not bot_response or len(bot_response) < 5:\n",
    "         bot_response = \"I understand your message. Could you provide more details so I can give you a better answer?\"\n",
    "    \n",
    "    # Format the response to reflect the guidelines given in the prompts\n",
    "    bot_response = format_model_response(bot_response)\n",
    "    \n",
    "    return bot_response\n",
    "\n",
    "def format_model_response(response):\n",
    "    \"\"\"Format model response to align with the assistant's guidelines.\"\"\"\n",
    "    # Clean up any artifacts in the generated text\n",
    "    response = re.sub(r'You: .*?Bot: ', '', response)\n",
    "    response = re.sub(r'Bot: ', '', response)\n",
    "    \n",
    "    # Format response with line breaks for readability\n",
    "    if len(response.split()) > 30:\n",
    "        sentences = response.split('. ')\n",
    "        if len(sentences) > 3:\n",
    "            formatted_response = '. '.join(sentences[:3]) + '.'\n",
    "            remaining = '. '.join(sentences[3:])\n",
    "            if remaining:\n",
    "                formatted_response += '\\n\\n' + remaining\n",
    "            response = formatted_response\n",
    "    \n",
    "    return response\n",
    "\n",
    "def send_message(event=None):\n",
    "    \"\"\"Process user input and generate a response.\"\"\"\n",
    "    message = user_input.get()\n",
    "    if message.strip() == \"\":\n",
    "        return\n",
    "    \n",
    "    # Clear the input field\n",
    "    user_input.delete(0, tk.END)\n",
    "    \n",
    "    # Display user message\n",
    "    chat_history.configure(state='normal')\n",
    "    chat_history.insert(tk.END, f\"You: {message}\\n\\n\", \"user_msg\")\n",
    "    chat_history.tag_configure(\"user_msg\", foreground=\"#2C3E50\", font=(\"Arial\", 12, \"bold\"))\n",
    "    \n",
    "    # Generate response\n",
    "    bot_message = analyze_and_generate_response(message)\n",
    "    \n",
    "    # Display bot message with slight delay for natural feel\n",
    "    root.after(300, lambda: display_bot_response(bot_message))\n",
    "    \n",
    "    # Add to conversation history\n",
    "    conversation_history.append(f\"Bot: {bot_message}\")\n",
    "\n",
    "def display_bot_response(message):\n",
    "    \"\"\"Display the bot's response in the chat history.\"\"\"\n",
    "    chat_history.configure(state='normal')\n",
    "    chat_history.insert(tk.END, \"Assistant: \", \"bot_tag\")\n",
    "    chat_history.tag_configure(\"bot_tag\", foreground=\"#2980B9\", font=(\"Arial\", 12, \"bold\"))\n",
    "    chat_history.insert(tk.END, f\"{message}\\n\\n\", \"bot_msg\")\n",
    "    chat_history.tag_configure(\"bot_msg\", foreground=\"#2980B9\", font=(\"Arial\", 12))\n",
    "    chat_history.see(tk.END)\n",
    "    chat_history.configure(state='disabled')\n",
    "\n",
    "# Bind the send_message function to the Return key\n",
    "user_input.bind('<Return>', send_message)\n",
    "\n",
    "# Create a Send button\n",
    "send_button = tk.Button(input_frame, text=\"Send\", command=send_message, bg=\"#3498DB\", fg=\"white\", font=(\"Arial\", 12, \"bold\"), padx=10)\n",
    "send_button.pack(side=tk.RIGHT, padx=10, pady=10)\n",
    "\n",
    "# Create a function to clear the chat history\n",
    "def clear_chat():\n",
    "    global conversation_history\n",
    "    conversation_history = []\n",
    "    chat_history.configure(state='normal')\n",
    "    chat_history.delete(1.0, tk.END)\n",
    "    chat_history.configure(state='disabled')\n",
    "\n",
    "# Create a Clear button\n",
    "clear_button = tk.Button(root, text=\"Clear Chat\", command=clear_chat, bg=\"#E74C3C\", fg=\"white\", font=(\"Arial\", 10))\n",
    "clear_button.pack(pady=5)\n",
    "\n",
    "# Add an info message at the start\n",
    "chat_history.configure(state='normal')\n",
    "welcome_message = \"Welcome! I'm an AI assistant designed to help answer your questions across various domains including science, history, technology, arts, and everyday advice. Feel free to ask me anything, and I'll do my best to assist you.\\n\\n\"\n",
    "chat_history.insert(tk.END, welcome_message, \"info_msg\")\n",
    "chat_history.tag_configure(\"info_msg\", foreground=\"#7F8C8D\", font=(\"Arial\", 11, \"italic\"))\n",
    "chat_history.configure(state='disabled')\n",
    "\n",
    "# Start the main loop\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8faafb-d32e-41da-9602-9f5f9fd608c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3883ba38-95d9-4ceb-b400-c59f6106882a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
